
# Introduction

Data are a fundamental currency upon which scientific discoveries are made. Without access to good data, it becomes extremely difficult to advance science. Yet, much of the data on which research papers are based are never visible to anyone beyond the original authors regardless of how much time passes after publications(needs citation). Sharing datasets upon publication benefits both the authoring researchers through increased visibility, and also the larger scientific community (McKiernan et al 2016). Assuming that exclusive access to the data doesn't confer any special advantage to the authors, or if there are no special privacy concerns, making data publicly and easily accessible can immediately increase the odds that a reader can re-run the entire computational workflow and reproduce the  results. The act of easy data sharing can also improves model training, as many different models can be tried and tested on latest data sources, closing the loop on research and application of statistical techniques. Existing datasets can be combined or linked with new or existing data, fostering the development and synthesis of new ideas and research areas. However, perhaps the greatest benefit of making data easily available are its impacts on reproducibility.

For nearly two decades, researchers who work in areas related to computational science have pushed for better standards to verify scientific claims, especially in areas where a full replication of the study would be prohibitively expensive. To meet these minimal standards, there must be easy access to the data, models, and the code and software used. The bioinformatics community has a strong culture around open source (cite https://genomebiology.biomedcentral.com/articles/10.1186/gb-2004-5-10-r80), and has made releasing software a mainstream activity. Many journals in these fields have also pushed authors to submit code (model implementations) alongside their papers, with a few journals going as far as providing a "reproducibility review" (cite http://science.sciencemag.org/content/334/6060/1226/tab-pdf).

The past decade has seen the rise of Data Science as a discipline distinct from statistics [@Donoho2017]. This has further cemented the role of free open source software (FOSS) tools as the key infrastructure blocks of reproducibility, and FOSS languages such as Python, R, and Julia are developing at a rapid rate. Community contributions to these languages are increasing at a tremendous rate, with many software packages being submitted to their respective package managers. As of 2018, PyPi (the Python package index) has > 125k packages, The Central R Archive Network (CRAN) has over 13k packages, with ~2k packages for Julia. These open source packages form the building blocks of a data scientists daily workflow.

Data analysis notebooks typically load a handful of libraries at the start, which provide all the routines necessary to read, transform, analyze, visualize and model data. Software packages are often built upon a complex layer of dependencies and provide new functionality for users. Data dependencies on the other hand are not as simple, and pose numerous challenges even to experienced data scientists. Why? Because data are far less standardized compared to software packages.

Here is a typical workflow for a typical data scientist for one of these problems:

* Files are read in from various locations.
* Different file formats are parsed
* Different data structures are standardized
* Data is wranged into a format suitable for data analysis
* Data is visualized
* Data is modeled.

There are three challenges that need to be managed to fully reproduce a data analysis: (1) The computing environment, (2), the code, and (3), the data.

The first two challenges are mostly solved. The computing environment is easily captured with modern tools such as Docker [@docker; @binder]. Modern tools such as Binder (citation) can parse Docker files and dependency trees to provide on demand, live notebooks in R and Python that a reader can immediately execute in the browser without any local installation. This makes it simple to load a specific environment to run any analysis. Code is handled by version control with tools like Git and GitHub [@git; @github], paired with archiving such as Zenodo and figshare provide access to code (particularly model implementations)[@zenodo; @figshare]. All the necessary software is available from various package managers (and their numerous geographic mirrors and archives) making it easy to install any version of a software package. However, the biggest challenge, even today, remains easy and repeatable access to data in a data science notebook.

While there are several papers that serve as best-practice guides for formatting data and getting them ready for sharing, the aims of this paper are a bit different. Our aim to address the issue of data in the context of reproducibility in data science workflows. In particular we discuss the various tradeoffs one has to consider depending on the nature of the data and how it is expected to be used. We also provide concrete examples on when to invest time and resources in preparing and documenting data extensively and when it is ok to skip these steps and choose an easier approach.  We discuss how to publish data, and detail the technical issues related to this, as well as challenges on licensing, and data embargoes (especially for sensitive data). We also analyze the state of data, particularly how it is packaged, shared and made available as part of modern data analysis. How often are data shipped inside software packages? How does data format and size impact availability? Is this problem better solved in one or more of these popular data science languages?
