
# Introduction

Data are a fundamental currency upon which scientific discoveries are made. Without access to good data, it becomes extremely difficult to advance science. Yet, much of the data on which research papers are based are never made available to anyone beyond the original authors (needs citation). Sharing datasets upon publication benefits both the authoring researchers, and the larger scientific community. Access to a greater diversity of data increases transparency and reproducibility. It also improves model training, as many different models can be tried and tested on latest data sources, closing the loop on research and application of statistical techniques. Existing datasets can be combined or linked with new or existing data, fostering the development and synthesis of new ideas and research areas. However, perhaps the greatest benefit of making data easily available are its impacts on reproducibility.

For nearly two decades, researchers who work in areas related to computational science have pushed for better standards to verify scientific claims, especially in areas where a full replication of the study would be prohibitively expensive. For such a minimal standard to be met, one would need access to the code/software tools, data, and the models.  At least in the bioinformatics community, a strong culture around open source (cite https://genomebiology.biomedcentral.com/articles/10.1186/gb-2004-5-10-r80) has made releasing software a mainstream activity. Many journals in these fields have also pushed authors to submit code (model implementations) alongside their papers, and a few journals have even gone as far as providing a "reproducibility review" (cite http://science.sciencemag.org/content/334/6060/1226/tab-pdf). Over the past decade alone, the rise of Data Science as a discipline distinct from statistics (cite Donoho 50 years) has further cemented the role of open source software tools as the key infrastructure blocks of reproducibility. These languages, Python, R and to a smaller extent Julia have seen a tremendous rise in the number of software packages that have been submitted to the respective package managers. As of 2018, PyPi (the Python package index) has > 125k packages, The Central R Archive Network (CRAN) has over 17k packages, with ~2k packages for Julia. These open source packages form the building blocks of a data scientists daily workflow. Data analysis notebooks typically a handful of libraries at the start, which provide all the routines necessary to load, transform, analyze, visualize and model data. Software packages call depend on other packages (within and across languages) and provide new functionality for users. Data dependencies on the other hand are not as simple and pose numerous challenges even to experienced data scientists. For starters, data are far less standardized compared to software packages. A typical data scientist will have to rely on various custom scripts to read files from various locations, parse numerous file formats, standardize across different data structures and wrangle the data into a usable format before they can be visualized and modeled.  For an analysis to be fully reproducible, one would need access to the computing environment, the code and the data.

The first two challenges are mostly solved. Modern tools such as  Docker and binder make it easy to capture the computational environment, making it easy for future users to load a specific environment to run older analysis. Services a Git and GitHub, paired with archives such as Zenodo and figshare provide access to code (particularly model implementations). All the necessary software is available from various package managers (and their numerous geographic mirrors and archives) making it easy to install any version of a software package. However, the biggest challenge, even today, remains easy and repeatable access to data in a data science notebook.


In this paper we analyze the state of data, particularly how it is packaged, shared and made available as part of modern data analysis compendiums. How often are data shipped inside software packages? How does data format and size impact availability? Is this problem better solved in one or more of these popular data science languages? We then provide guidance on how a data scientist can package and ship their data to maximize reproducibility. In addition to technical issues, we also cover challenges such as licensing, data embargos (especially for sensitive data), and advice on how to publish these datasets.
